# --- Stage 1: Python Base Image ---
FROM python:3.11-slim

# Set the working directory inside the container
WORKDIR /app

# Install OS-level build tools
RUN apt-get update && apt-get install -y \
    build-essential \
    cmake \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*
    
# --- Install Dependencies ---
# Copy ONLY the requirements file first to leverage Docker caching
COPY requirements.txt .

# --- CPU-ONLY INSTALLATION ---
# Remove all GPU-related build arguments (CMAKE_ARGS, FORCE_CMAKE)
# This will build llama-cpp-python for CPU only.
# ---
RUN pip install --upgrade pip && \
    pip install -r requirements.txt --no-cache-dir

# --- Copy Application Code ---
# Copy the rest of your backend application code
COPY . /app

# --- Expose Port & Run ---
# Expose the port FastAPI will run on
EXPOSE 8000

# Run the application using Uvicorn
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]